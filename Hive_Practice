*****************************************************************************************************
Default Location: /user/hive/warehouse/
set hive.cli.print.current.db=true;
set hive.cli.print.header=true;
set hive.exec.dynamic.partition.mode=nonstrict;
*****************************************************************************************************
				****Load data from Local to Hive*****
*****************************************************************************************************
1) Managed Table:

create table movies_tbl
(
movieId int,
title string,
genres string
) 
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
tblproperties('skip.header.line.count'='1');

load data local inpath '/home/labuser/LFS/datasets/movies.csv' into table movies_tbl;
*****************************************************************************************************
				*****Load data from Hadoop to Hive*****
*****************************************************************************************************
create table movies_tbl1
(
movieId int,
title string,
genres string
) 
row format delimited fields terminated by ','
tblproperties('skip.header.line.count'='1');

hdfs dfs -copyFromLocal movies.csv /user/labuser/HFS/Input/
load data inpath '/user/labuser/HFS/Input/movies.csv' into table movies_tbl1;
*****************************************************************************************************
				*****External Tables*****
*****************************************************************************************************
2) External Tables:

create external table ext_movies_tbl
(
movieId int,
title string,
genres string
)
row format delimited fields terminated by ','
tblproperties('skip.header.line.count'='1');

load data inpath '/user/labuser/HFS/Input/movies.csv' into table ext_movies_tbl;
load data inpath '/user/labuser/HFS/Input/movies.csv' into table ext_movies_tbl;

load data inpath '/user/labuser/HFS/Input/movies.csv' overwrite into table ext_movies_tbl;
*****************************************************************************************************
				*****Location*****
*****************************************************************************************************
create external table ext_movies_tbl
(
movieId int,
title string,
genres string
)
row format delimited fields terminated by ','
location '/user/hive/warehouse/saif_db.db/ext_movies_tbl/'
tblproperties('skip.header.line.count'='1');
*****************************************************************************************************
				*****Complex Data Type: Array*****
*****************************************************************************************************
create table array_tbl
(
id int,
name string,
sal int,
assets array<string>,
city string
)
row format delimited fields terminated by ','
collection items terminated by '$'
tblproperties('skip.header.line.count'='1');

hdfs dfs -copyFromLocal array_file.txt /user/labuser/HFS/Input/
load data inpath '/user/labuser/HFS/Input/array_file.txt' into table array_tbl;

hdfs dfs -mkdir /user/labuser/HFS/Input/array
hdfs dfs -put array_file /user/labuser/HFS/Input/array/

create table array_tbl_loc
(
id int,
name string,
sal int,
assets array<string>,
city string
)
row format delimited fields terminated by ','
collection items terminated by '$'
location '/user/labuser/HFS/Input/array/'
tblproperties('skip.header.line.count'='1');
*****************************************************************************************************
				*****Complex Data Type: Map*****
*****************************************************************************************************
create table map_tbl
(id int,
name string,
sal int,
lappy_info array<string>,
pf_info map<string, int>,
city string)
row format delimited fields terminated by ','
collection items terminated by '$'
map keys terminated by '#'
tblproperties('skip.header.line.count'='1');

hdfs dfs -copyFromLocal map_file.txt /user/labuser/HFS/Input/
load data inpath '/user/labuser/HFS/Input/map_file.txt' into table map_tbl;
*****************************************************************************************************
				*****Complex Data Type: Struct*****
*****************************************************************************************************
create table struct_tbl
(
id int,
name string,
sal int,
subject array<string>, 
deduction map<string, int>,
address struct<state:string, city: string, pincode: int>
)
row format delimited fields terminated by ','
collection items terminated by '$'
map keys terminated by '#'
location '/user/labuser/HFS/Input/struct/'
tblproperties('skip.header.line.count'='1');

select subject[1] as subject, deduction['epf'] as deduction, address.state as address from struct_tbl;
************************************************************************************************
				*****Static Partitions*****
************************************************************************************************
1st Approach: When Client sents data in different files:
create table stc_prtn
(
id int,
name string,
sal int
)
partitioned by (country string)
row format delimited fields terminated by ','
tblproperties('skip.header.line.count'='1');

load data local inpath '/home/labuser/LFS/datasets/emp_ind.txt' into table stc_prtn partition (country='IN');
load data local inpath '/home/labuser/LFS/datasets/emp_uk.txt' into table stc_prtn partition (country='UK');
load data local inpath '/home/labuser/LFS/datasets/emp_us.txt' into table stc_prtn partition (country='US');

2nd Approach: Client is sending all the data in 1 file
create table stc_prtn_all
(
id int,
name string,
sal int
)
partitioned by (country string)
row format delimited fields terminated by ','
tblproperties('skip.header.line.count'='1');

load data local inpath '/home/labuser/LFS/datasets/emp_partition_all.txt' into table stc_prtn_all partition (country='IN');

Problem:
All countries data is loaded into one partition.

Solution:
1) Create Temp/Stg Table
2) Load Data into Temp/Stg table
3) Create Partitioned Table
4) Load Data into Partitioned Table from Temp/Stg

create table stg_country
(
id int,
name string,
sal int,
country string
)
row format delimited fields terminated by ','
tblproperties('skip.header.line.count'='1');

load data local inpath '/home/labuser/LFS/datasets/emp_partition_all.txt' into table stg_country;

create table stc_prtn_all_1
(
id int,
name string,
sal int
)
partitioned by (country string)
row format delimited fields terminated by ',';

insert into table stc_prtn_all_1 partition (country='IN') select id, name, sal from stg_country where country = 'IN';
insert into table stc_prtn_all_1 partition (country='UK') select id, name, sal from stg_country where country = 'UK';
insert into table stc_prtn_all_1 partition (country='US') select id, name, sal from stg_country where country = 'US';
************************************************************************************************
				*****Dynamic Partitions*****
************************************************************************************************
1) Create Temp/Stg Table		==> Already Created ==> stg_country
2) Load Data into Temp/Stg table	==> Alread Loaded   ==> select * from stg_country;
3) Create Partitioned Table		
4) Load Data into Partitioned Table from Temp/Stg

create table dyn_prtn_all
(
id int,
name string,
sal int
)
partitioned by (country string)
row format delimited fields terminated by ',';

insert into table dyn_prtn_all partition (country) select id, name, sal, country from stg_country;

Enable this property:
set hive.exec.dynamic.partition.mode=nonstrict;
************************************************************************************************
					*****Bucketing*****
************************************************************************************************
create table stg_emp_bucket
(
street string,
city string,
zip int,
state string,
beds int,
baths int,
sq_ft int,
type string,
price int
)
row format delimited fields terminated by ','
tblproperties("skip.header.line.count"="1");

load data local inpath '/home/labuser/LFS/datasets/emp_bucket.txt' into table stg_emp_bucket;

create table emp_bucket
(
street string,
zip int,
state string,
beds int,
baths int,
sq_ft int,
type string,
price int
)
partitioned by (city string)
clustered by (street) into 4 buckets
row format delimited fields terminated by ',';

insert into table emp_bucket partition (city) select street, zip, state, beds, baths, sq_ft, type, price, city from stg_emp_bucket;

Table Sampling:
select * from emp_bucket limit 10;
select * from emp_bucket tablesample (bucket 1 out of 4) limit 10;
select * from emp_bucket tablesample (bucket 1 out of 4 on street) limit 10;
************************************************************************************************
					*****Joins*****
************************************************************************************************
--> customers.txt
ID,Name,Age,Address,Salary
1,Ross,32,Ahmedabad,2000
2,Rachel,25,Delhi,1500
3,Chandler,23,Kota,2000
4,Monika,25,Mumbai,6500
5,Mike,27,Bhopal,8500
6,Phoebe,22,MP,4500
7,Joey,24,Indore,10000

--> customers_join
create table if not exists customers_join
(
id int,
name string,
age int,
address string,
salary int
)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
tblproperties("skip.header.line.count"="1");

load data local inpath '/home/cloudera/LFS/customers.txt' into table customers_join;

--> orders.txt
OID,Date,Customer_ID,Amount
102,2016-10-08 00:00:00,3,3000
100,2016-10-08 00:00:00,3,1500
101,2016-11-20 00:00:00,2,1560
103,2015-05-20 00:00:00,4,2060

--> orders_join
create table if not exists orders_join
(
oid int,
ord_date timestamp,
customer_id int,
amount int
)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
tblproperties("skip.header.line.count"="1");

load data local inpath '/home/cloudera/LFS/orders.txt' into table orders_join;

1) Inner Join:
select c.id, c.name, c.age, o.amount
from customers_join c join orders_join o
on c.id = o.customer_id;

2) Left Outer Join:
select c.id, c.name, o.amount, o.ord_date
from customers_join c
left outer join orders_join o
on c.id = o.customer_id;

3) Right Outer Join:
select c.id, c.name, o.amount, o.ord_date from customers_join c
right outer join orders_join o
on c.id = o.customer_id;

4) Full Outer Join:
select c.id, c.name, o.amount, o.ord_date
from customers_join c
full outer join orders_join o
on c.id = o.customer_id;

--> order_item.txt
oid,ord_date,items,amount
102,2016-10-08 00:00:00,Pizza,3000
102,2016-10-08 00:00:00,Juice,3000
100,2016-10-08 00:00:00,Biryani,1500
101,2016-11-20 00:00:00,Paneer,1560
103,2015-05-20 00:00:00,Momos,2060

create table if not exists order_item_join
(
oid int,
ord_date timestamp,
items string,
amount int
)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
location '/user/cloudera/HFS/Input/order_item/'
tblproperties("skip.header.line.count"="1");

hdfs dfs -put order_item.txt /user/cloudera/HFS/Input/order_item/

5) Joining 3 tables: First 2 tables with Inner join and the result set with the 3rd table via left outer join.
select c.id, c.name, c.age, o.amount, oi.items
from customers_join c join orders_join o
on c.id = o.customer_id
left outer join order_item_join oi
on oi.oid=o.oid;

View:
CREATE VIEW IF NOT EXISTS V1 AS
select c.id, c.name, c.age, o.amount, oi.items
from customers_join c join orders_join o
on c.id = o.customer_id
left outer join order_item_join oi
on oi.oid=o.oid;

SELECT * FROM V1;
************************************************************************************************
					*****ACID Properties*****
************************************************************************************************
create external table if not exists emp_acid
(
id int,
name string,
sal int,
city string
)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
tblproperties("skip.header.line.count"="1");

load data local inpath '/home/saif/LFS/emp_acid.txt' into table emp_acid;

set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.support.concurrency=true;
set hive.enforce.bucketing=true;
set hive.exec.dynamic.partition.mode=nonstrict;

If needed:
set hive.compactor.initiator.on=true;
set hive.compactor.worker.threads=1;

create table if not exists emp_acid_orc
(
id int,
name string,
sal int,
city string
)
clustered by (id) into 4 buckets
row format delimited fields terminated by ','
lines terminated by '\n'
stored as orc
tblproperties("transactional"="true");

Load Data from stg to ACID Tables:
insert overwrite table emp_acid_orc select * from emp_acid;
select * from emp_acid_orc;

Insert:
insert into emp_acid_orc values (104,'vazir',400,'pune');
Update:
update emp_acid_orc set sal = 999 where id = 101;
Delete:
delete from emp_acid_orc where id = 104;

