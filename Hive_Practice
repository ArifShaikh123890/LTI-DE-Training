*****************************************************************************************************
Default Location: /user/hive/warehouse/
set hive.cli.print.current.db=true;
set hive.cli.print.header=true;
set hive.exec.dynamic.partition.mode=nonstrict;
*****************************************************************************************************
				****Load data from Local to Hive*****
*****************************************************************************************************
1) Managed Table:

create table movies_tbl
(
movieId int,
title string,
genres string
) 
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
tblproperties('skip.header.line.count'='1');

load data local inpath '/home/labuser/LFS/datasets/movies.csv' into table movies_tbl;
*****************************************************************************************************
				*****Load data from Hadoop to Hive*****
*****************************************************************************************************
create table movies_tbl1
(
movieId int,
title string,
genres string
) 
row format delimited fields terminated by ','
tblproperties('skip.header.line.count'='1');

hdfs dfs -copyFromLocal movies.csv /user/labuser/HFS/Input/
load data inpath '/user/labuser/HFS/Input/movies.csv' into table movies_tbl1;
*****************************************************************************************************
				*****External Tables*****
*****************************************************************************************************
2) External Tables:

create external table ext_movies_tbl
(
movieId int,
title string,
genres string
)
row format delimited fields terminated by ','
tblproperties('skip.header.line.count'='1');

load data inpath '/user/labuser/HFS/Input/movies.csv' into table ext_movies_tbl;
load data inpath '/user/labuser/HFS/Input/movies.csv' into table ext_movies_tbl;

load data inpath '/user/labuser/HFS/Input/movies.csv' overwrite into table ext_movies_tbl;
*****************************************************************************************************
				*****Location*****
*****************************************************************************************************
create external table ext_movies_tbl
(
movieId int,
title string,
genres string
)
row format delimited fields terminated by ','
location '/user/hive/warehouse/saif_db.db/ext_movies_tbl/'
tblproperties('skip.header.line.count'='1');
*****************************************************************************************************
				*****Complex Data Type: Array*****
*****************************************************************************************************
create table array_tbl
(
id int,
name string,
sal int,
assets array<string>,
city string
)
row format delimited fields terminated by ','
collection items terminated by '$'
tblproperties('skip.header.line.count'='1');

hdfs dfs -copyFromLocal array_file.txt /user/labuser/HFS/Input/
load data inpath '/user/labuser/HFS/Input/array_file.txt' into table array_tbl;

hdfs dfs -mkdir /user/labuser/HFS/Input/array
hdfs dfs -put array_file /user/labuser/HFS/Input/array/

create table array_tbl_loc
(
id int,
name string,
sal int,
assets array<string>,
city string
)
row format delimited fields terminated by ','
collection items terminated by '$'
location '/user/labuser/HFS/Input/array/'
tblproperties('skip.header.line.count'='1');
*****************************************************************************************************
				*****Complex Data Type: Map*****
*****************************************************************************************************
create table map_tbl
(id int,
name string,
sal int,
lappy_info array<string>,
pf_info map<string, int>,
city string)
row format delimited fields terminated by ','
collection items terminated by '$'
map keys terminated by '#'
tblproperties('skip.header.line.count'='1');

hdfs dfs -copyFromLocal map_file.txt /user/labuser/HFS/Input/
load data inpath '/user/labuser/HFS/Input/map_file.txt' into table map_tbl;
*****************************************************************************************************
				*****Complex Data Type: Struct*****
*****************************************************************************************************
create table struct_tbl
(
id int,
name string,
sal int,
subject array<string>, 
deduction map<string, int>,
address struct<state:string, city: string, pincode: int>
)
row format delimited fields terminated by ','
collection items terminated by '$'
map keys terminated by '#'
location '/user/labuser/HFS/Input/struct/'
tblproperties('skip.header.line.count'='1');

select subject[1] as subject, deduction['epf'] as deduction, address.state as address from struct_tbl;
************************************************************************************************
				*****Static Partitions*****
************************************************************************************************
1st Approach: When Client sents data in different files:
create table stc_prtn
(
id int,
name string,
sal int
)
partitioned by (country string)
row format delimited fields terminated by ','
tblproperties('skip.header.line.count'='1');

load data local inpath '/home/labuser/LFS/datasets/emp_ind.txt' into table stc_prtn partition (country='IN');
load data local inpath '/home/labuser/LFS/datasets/emp_uk.txt' into table stc_prtn partition (country='UK');
load data local inpath '/home/labuser/LFS/datasets/emp_us.txt' into table stc_prtn partition (country='US');

2nd Approach: Client is sending all the data in 1 file
create table stc_prtn_all
(
id int,
name string,
sal int
)
partitioned by (country string)
row format delimited fields terminated by ','
tblproperties('skip.header.line.count'='1');

load data local inpath '/home/labuser/LFS/datasets/emp_partition_all.txt' into table stc_prtn_all partition (country='IN');

Problem:
All countries data is loaded into one partition.

Solution:
1) Create Temp/Stg Table
2) Load Data into Temp/Stg table
3) Create Partitioned Table
4) Load Data into Partitioned Table from Temp/Stg

create table stg_country
(
id int,
name string,
sal int,
country string
)
row format delimited fields terminated by ','
tblproperties('skip.header.line.count'='1');

load data local inpath '/home/labuser/LFS/datasets/emp_partition_all.txt' into table stg_country;

create table stc_prtn_all_1
(
id int,
name string,
sal int
)
partitioned by (country string)
row format delimited fields terminated by ',';

insert into table stc_prtn_all_1 partition (country='IN') select id, name, sal from stg_country where country = 'IN';
insert into table stc_prtn_all_1 partition (country='UK') select id, name, sal from stg_country where country = 'UK';
insert into table stc_prtn_all_1 partition (country='US') select id, name, sal from stg_country where country = 'US';
************************************************************************************************
				*****Dynamic Partitions*****
************************************************************************************************
1) Create Temp/Stg Table		==> Already Created ==> stg_country
2) Load Data into Temp/Stg table	==> Alread Loaded   ==> select * from stg_country;
3) Create Partitioned Table		
4) Load Data into Partitioned Table from Temp/Stg

create table dyn_prtn_all
(
id int,
name string,
sal int
)
partitioned by (country string)
row format delimited fields terminated by ',';

insert into table dyn_prtn_all partition (country) select id, name, sal, country from stg_country;

Enable this property:
set hive.exec.dynamic.partition.mode=nonstrict;
************************************************************************************************
					*****Bucketing*****
************************************************************************************************
create table stg_emp_bucket
(
street string,
city string,
zip int,
state string,
beds int,
baths int,
sq_ft int,
type string,
price int
)
row format delimited fields terminated by ','
tblproperties("skip.header.line.count"="1");

load data local inpath '/home/labuser/LFS/datasets/emp_bucket.txt' into table stg_emp_bucket;

create table emp_bucket
(
street string,
zip int,
state string,
beds int,
baths int,
sq_ft int,
type string,
price int
)
partitioned by (city string)
clustered by (street) into 4 buckets
row format delimited fields terminated by ',';

insert into table emp_bucket partition (city) select street, zip, state, beds, baths, sq_ft, type, price, city from stg_emp_bucket;

Table Sampling:
select * from emp_bucket limit 10;
select * from emp_bucket tablesample (bucket 1 out of 4) limit 10;
select * from emp_bucket tablesample (bucket 1 out of 4 on street) limit 10;
************************************************************************************************
					*****Joins*****
************************************************************************************************
--> customers.txt
ID,Name,Age,Address,Salary
1,Ross,32,Ahmedabad,2000
2,Rachel,25,Delhi,1500
3,Chandler,23,Kota,2000
4,Monika,25,Mumbai,6500
5,Mike,27,Bhopal,8500
6,Phoebe,22,MP,4500
7,Joey,24,Indore,10000

--> customers_join
create table if not exists customers_join
(
id int,
name string,
age int,
address string,
salary int
)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
tblproperties("skip.header.line.count"="1");

load data local inpath '/home/cloudera/LFS/customers.txt' into table customers_join;

--> orders.txt
OID,Date,Customer_ID,Amount
102,2016-10-08 00:00:00,3,3000
100,2016-10-08 00:00:00,3,1500
101,2016-11-20 00:00:00,2,1560
103,2015-05-20 00:00:00,4,2060

--> orders_join
create table if not exists orders_join
(
oid int,
ord_date timestamp,
customer_id int,
amount int
)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
tblproperties("skip.header.line.count"="1");

load data local inpath '/home/cloudera/LFS/orders.txt' into table orders_join;

1) Inner Join:
select c.id, c.name, c.age, o.amount
from customers_join c join orders_join o
on c.id = o.customer_id;

2) Left Outer Join:
select c.id, c.name, o.amount, o.ord_date
from customers_join c
left outer join orders_join o
on c.id = o.customer_id;

3) Right Outer Join:
select c.id, c.name, o.amount, o.ord_date from customers_join c
right outer join orders_join o
on c.id = o.customer_id;

4) Full Outer Join:
select c.id, c.name, o.amount, o.ord_date
from customers_join c
full outer join orders_join o
on c.id = o.customer_id;

--> order_item.txt
oid,ord_date,items,amount
102,2016-10-08 00:00:00,Pizza,3000
102,2016-10-08 00:00:00,Juice,3000
100,2016-10-08 00:00:00,Biryani,1500
101,2016-11-20 00:00:00,Paneer,1560
103,2015-05-20 00:00:00,Momos,2060

create table if not exists order_item_join
(
oid int,
ord_date timestamp,
items string,
amount int
)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
location '/user/cloudera/HFS/Input/order_item/'
tblproperties("skip.header.line.count"="1");

hdfs dfs -put order_item.txt /user/cloudera/HFS/Input/order_item/

5) Joining 3 tables: First 2 tables with Inner join and the result set with the 3rd table via left outer join.
select c.id, c.name, c.age, o.amount, oi.items
from customers_join c join orders_join o
on c.id = o.customer_id
left outer join order_item_join oi
on oi.oid=o.oid;

View:
CREATE VIEW IF NOT EXISTS V1 AS
select c.id, c.name, c.age, o.amount, oi.items
from customers_join c join orders_join o
on c.id = o.customer_id
left outer join order_item_join oi
on oi.oid=o.oid;

SELECT * FROM V1;
************************************************************************************************
				*****ACID Properties*****
************************************************************************************************
1) Tables should be orc
2) Tables should be bucketed
3) Transactional Property shoule be set to true ==> "transactional"="true"
4) Tables cannot be external
5) Once Acidic tables are created they cannot be reverted to non-acidic
6) In a session if you have created Acidic tables you cannot access those tables from other sessions
7) We cannot use load command to load data
8) INSERT OVERWRITE not allowed on table with OutputFormat that implements AcidOutputFormat while transaction manager that supports ACID is in use

create external table stg_emp_acid
(
id int,
name string,
sal int,
city string
)
row format delimited fields terminated by ','
tblproperties("skip.header.line.count"="1");

load data local inpath '/home/labuser/LFS/emp_acid.txt' into table stg_emp_acid;

set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.support.concurrency=true;
set hive.enforce.bucketing=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table emp_acid_orc
(
id int,
name string,
sal int,
city string
)
clustered by (id) into 4 buckets
row format delimited fields terminated by ','
stored as orc
tblproperties("transactional"="true");

Load Data from stg to ACID Tables:
insert overwrite table emp_acid_orc select * from stg_emp_acid;
It gives error: overwrite is not allowed

insert into table emp_acid_orc select * from stg_emp_acid;
select * from emp_acid_orc;

Insert:
insert into emp_acid_orc values (104,'vazir',400,'pune');
Update:
update emp_acid_orc set sal = 999 where id = 101;
Delete:
delete from emp_acid_orc where id = 104;


